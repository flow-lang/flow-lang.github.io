(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{158:function(t,e,n){"use strict";var o={props:{colour:{type:String}},data:function(){return{blockquoteClass:"{-background bg-".concat(this.colour,"-100 -} {-border border-l-4 border-").concat(this.colour,"-500 -} {-spacing my-4 p-4 -}")}}},r=n(17),component=Object(r.a)(o,function(){var t=this.$createElement;return(this._self._c||t)("blockquote",{class:this.blockquoteClass},[this._t("default")],2)},[],!1,null,null,null);e.a=component.exports},159:function(t,e,n){"use strict";var o={data:function(){return{preClass:"{-display overflow-x-scroll -} {-typography text-sm -} {-background bg-gray-200 -} {-border border-l-4 rounded-r-lg border-gray-600 -} {-sizing -} {-spacing my-4 p-2 -}"}}},r=n(17),component=Object(r.a)(o,function(){var t=this.$createElement,e=this._self._c||t;return e("pre",{class:this.preClass},[this._v("  "),e("code",[this._t("default")],2),this._v("\n")])},[],!1,null,null,null);e.a=component.exports},160:function(t,e,n){"use strict";var o=n(17),component=Object(o.a)({},function(){var t=this.$createElement;return(this._self._c||t)("section",{staticClass:"{-spacing pb-4 -}"},[this._t("default")],2)},[],!1,null,null,null);e.a=component.exports},161:function(t,e,n){"use strict";var o=n(17),component=Object(o.a)({},function(){var t=this.$createElement,e=this._self._c||t;return e("div",[e("h2",{staticClass:"{-typography text-xl -} {-spacing pt-6 pb-2"},[this._t("default")],2),this._v(" "),e("hr",{staticClass:"{-border border-b border-gray-400 -} {-spacing mb-4 -}"})])},[],!1,null,null,null);e.a=component.exports},162:function(t,e,n){"use strict";var o=n(17),component=Object(o.a)({},function(){var t=this.$createElement;return(this._self._c||t)("h1",{staticClass:"{-typography text-3xl -}"},[this._t("default")],2)},[],!1,null,null,null);e.a=component.exports},193:function(t,e,n){"use strict";n.r(e);var o=n(162),r=n(161),c=n(160),l=n(159),d=n(158),h={components:{ContentHeading:o.a,ContentSubHeading:r.a,ContentSection:c.a,ContentCode:l.a,ContentNotice:d.a}},m=n(17),component=Object(m.a)(h,function(){var t=this,e=t.$createElement,n=t._self._c||e;return n("div",[n("content-heading",[t._v("Introduction")]),t._v(" "),n("content-section",[n("content-sub-heading",[t._v("What is Flow?")]),t._v("\n    Flow is a complete front-end framework for developing interactive audio applications\n    with the Web Audio API. It has been built from the ground up to provide a unified,\n    declarative API for building both HTML interfaces and audio processing graphs.\n\n    If you'd like to read a more technical overview of the framework, you can\n    read our paper "),n("a",{attrs:{href:""}},[t._v('"A Model-View-Update Framework for Interactive Audio Web Applications".')]),t._v("\n\n    If you're an experienced Web Audio developer and are curious how Flow compares\n    to other Web Audio libraries such as tone.js or the Web Audio API eXtension,\n    check out the Comparison to Other Frameworks.\n    "),n("content-notice",{attrs:{colour:"red"}},[t._v("\n      The follwing documentation assumes a familiarity with ES6 features such as\n      arrow functions and object/array destructuring, as well as a basic understanding\n      of functional javascript.\n    ")])],1),t._v(" "),n("content-section",[n("content-sub-heading",[t._v("Declarative Programming")]),t._v("\n    The central idea behind declarative programming is adopting a style of programming\n    that says what not how something should happen. Consider the simple scenario\n    of creating an oscillator and setting some properties:\n    "),n("content-code",[t._v('\n      const audioContext = new (window.AudioContext || window.webkitAudioContext)()\n      const osc = audioContext.createOscillator()\n      \n      oscillator.type = "sine"\n      oscillator.frequency.value = 440\n      oscillator.connect(audioContext.destination)\n      oscillator.start()\n    ')]),t._v("\n    The traditional approach is store a new oscillator node in some variable, then\n    update that object's properties, and finally call the appropriate methods to\n    connect the node to the audio output and start it. This is known as procedural\n    programming where we take a step by step approach to computation.\n    "),n("content-code",[t._v('\n      oscillator([ type("sine"), frequency(440) ], [\n        dac()\n      ])\n    ')]),t._v("\n    The declarative API provided by Flow allows you to focus on the most important\n    parts of development; what you want to happen and the data you need for that.\n    We don't have to worry about creating and managing an audio context, and we\n    don't have to worry about variable names.\n  ")],1),t._v(" "),n("content-section",[n("content-sub-heading",[t._v("Pure Functions")]),t._v("\n    Flow puts a heavy emphasis on writing pure code. Pure functions are like the\n    functions you learn about in maths: same input equals same output, always.\n    This makes our programs predictable and decidable.\n\n    This goes hand in hand with declarative programming, because we're focused\n    on what we want to happen rather thanhow we can let the runtime handle managing\n    messy stateful actions like manipulating the DOM, scheduling audio events,\n    and reacting to user input.\n\n    Stateful actions become data that describe to the runtime how to do something.\n    This is incredibly powerful; that data can be passed around, manipulated, o\n    even shared with another program with ease.\n  ")],1),t._v(" "),n("content-section",[n("content-sub-heading",[t._v("A Quick Example")]),t._v("\n    In the next section you'll learn about the Flow architecture and how update,\n    audio, view, and listen come together to form a complete application, but for\n    now here is a typical counter app with an audio twist.\n    "),n("content-code",[t._v('\n      import { Program, Effect, Action, Audio, DOM } from "@flow/framework"\n\n      const { Element, Attribute, Event } = DOM\n      const { Node, Property } = Audio\n\n      const App = Program.instrument(init, update, audio, view, listen)\n\n      function init () {\n        return 1\n      }\n\n      function update ({ action }, model) {\n        switch (action) {\n          case "Increment":\n            return [model + 1, Effect.none()]\n          case "Decrement":\n            return [model - 1, Effect.none()]\n        }\n      }\n\n      function audio (model) {\n        return Node.oscillator([ Property.frequency(440) ], [\n          Node.gain([ Property.gain(model / 10) ], [\n            Node.dac()\n          ])\n        ])\n      }\n\n      function view (model) {\n        return Element.div([], [\n          Element.button([ Attribute.className(".inc") ], [ "+" ]),\n          Element.div([], [ model.toString() ]),\n          Element.button([ Attribute.className(".dec") ], [ "-" ])\n        ])\n      }\n\n      function listen (model) {\n        return [\n          Event.click(".inc", _ => Action("Increment")),\n          Event.click(".dec", _ => Action("Decrement"))\n        ]\n      }\n\n      App.use(Event)\n      App.start({ \n        content: new AudioContext(),\n        root: document.querySelector("#app") \n      })\n    ')]),t._v("\n    Notice how each element is entirely decoupled. The runtime is in charge of\n    slotting all these pieces together, so your application becomes a highly modular.\n    It's now trivial to try different views or different audio graphs without\n    wasting time hooking everything up.\n  ")],1),t._v(" "),n("div",{staticClass:"{-typography text-right text-indigo-500 hover:text-indigo-800 -} {-spacing my-4 -}"},[n("nuxt-link",{attrs:{to:"/the-flow-architecture"}},[t._v("The Flow Architecture")]),t._v("Â»\n  ")],1)],1)},[],!1,null,null,null);e.default=component.exports}}]);