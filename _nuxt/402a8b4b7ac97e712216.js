(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{158:function(n,e,t){"use strict";var o={props:{colour:{type:String}},data:function(){return{blockquoteClass:"{-background bg-".concat(this.colour,"-100 -} {-border border-l-4 border-").concat(this.colour,"-500 -} {-spacing my-4 p-4 -}")}}},r=t(17),component=Object(r.a)(o,function(){var n=this.$createElement;return(this._self._c||n)("blockquote",{class:this.blockquoteClass},[this._t("default")],2)},[],!1,null,null,null);e.a=component.exports},159:function(n,e,t){"use strict";var o={data:function(){return{preClass:"{-display overflow-x-scroll -} {-typography text-sm -} {-background bg-gray-200 -} {-border border-l-4 rounded-r-lg border-gray-600 -} {-sizing -} {-spacing my-4 p-2 -}"}}},r=t(17),component=Object(r.a)(o,function(){var n=this.$createElement,e=this._self._c||n;return e("pre",{class:this.preClass},[this._v("  "),e("code",[this._t("default")],2),this._v("\n")])},[],!1,null,null,null);e.a=component.exports},160:function(n,e,t){"use strict";var o=t(17),component=Object(o.a)({},function(){var n=this.$createElement;return(this._self._c||n)("section",{staticClass:"{-spacing pb-4 -}"},[this._t("default")],2)},[],!1,null,null,null);e.a=component.exports},161:function(n,e,t){"use strict";var o=t(17),component=Object(o.a)({},function(){var n=this.$createElement,e=this._self._c||n;return e("div",[e("h2",{staticClass:"{-typography text-xl -} {-spacing pt-6 pb-2"},[this._t("default")],2),this._v(" "),e("hr",{staticClass:"{-border border-b border-gray-400 -} {-spacing mb-4 -}"})])},[],!1,null,null,null);e.a=component.exports},162:function(n,e,t){"use strict";var o=t(17),component=Object(o.a)({},function(){var n=this.$createElement;return(this._self._c||n)("h1",{staticClass:"{-typography text-3xl -}"},[this._t("default")],2)},[],!1,null,null,null);e.a=component.exports},164:function(n,e,t){"use strict";var o=t(163),r=[];window.addEventListenerBase||(window.addEventListenerBase=window.addEventListener,window.addEventListener=function(n,e){r.push({type:n,listener:e}),window.addEventListenerBase(n,e)},window.removeEventListeners=function(n){for(var e=0;e!==r.length;e++){var t=r[e],o=t.listener,c=t.type;c===n&&window.removeEventListener(c,o)}}),e.a=function(){for(var n in o.DOM.Event.$events)window.removeEventListeners(n)}},165:function(n,e,t){"use strict";function o(n){"running"!==n.state&&(window.addEventListener("click",function(){"running"!==n.state&&n.resume(),console.log(n.state)}),window.addEventListener("touch",function(){"running"!==n.state&&n.resume(),console.log(n.state)}))}t.d(e,"a",function(){return o})},166:function(n,e){function t(n){var e=new Error("Cannot find module '"+n+"'");throw e.code="MODULE_NOT_FOUND",e}t.keys=function(){return[]},t.resolve=t,n.exports=t,t.id=166},184:function(n,e,t){"use strict";t.r(e);t(69),t(37),t(27),t(18),t(54);var o=t(49),r=t(163),c=t(164),l=t(165);function d(object,n){var e=Object.keys(object);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(object);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(object,n).enumerable})),e.push.apply(e,t)}return e}function f(n){for(var i=1;i<arguments.length;i++){var source=null!=arguments[i]?arguments[i]:{};i%2?d(source,!0).forEach(function(e){Object(o.a)(n,e,source[e])}):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(source)):d(source).forEach(function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(source,e))})}return n}var v=window.AudioContext||window.webkitAudioContext;function h(n){var e=function(n,e){return{key:n,freq:r.Music.Note.ntof(e),on:!1}};return{notes:[e("q","A3"),e("w","C4"),e("e","D4"),e("r","E4"),e("t","G4"),e("y","A4"),e("u","C5"),e("i","D5"),e("o","E5"),e("p","G5")]}}var y="note-on",w="note-off";function m(n,e){var t=n.action,o=n.payload;switch(t){case y:var r=o.key,c=e.notes.map(function(n){return n.key===r?f({},n,{on:!0}):f({},n)});return[f({},e,{notes:c}),null];case w:var l=o.key,d=e.notes.map(function(n){return n.key===l?f({},n,{on:!1}):f({},n)});return[f({},e,{notes:d}),null]}}var _=r.Audio.Node,k=r.Audio.Property,O=function(n){var e=n.on,t=n.freq,o=k.gain(e?.1:0),r=k.frequency(t);return _.oscillator([r],[_.gain([o],[_.dac()])])};function audio(n){return n.notes.map(O)}var E=r.DOM.Element,x=r.DOM.Attribute,C=function(n){var e=n.key,t=n.freq,o=n.on,c="\n    flex-1 text-center p-2 m-2 text-".concat(o?"indigo-700":"gray-400","\n    border border-").concat(o?"indigo-700":"gray-400","\n  ");return E.div([x.className(c)],[E.p([],[e]),E.p([],[r.Music.Note.fton(t)])])};function view(n){return E.div([x.className("flex")],n.notes.map(C))}var j,N,A=function(n){return function(e){return Object(r.Action)(n,{key:e.key.toLowerCase()})}};function D(n){return[r.DOM.Event.keydown("window",A(y)),r.DOM.Event.keyup("window",A(w))]}var M=function(n){N=new v,Object(l.a)(N),(j=r.Program.instrument(h,m,audio,view,D)).use(r.DOM.Event),j.use(r.Audio.Event),j.start({root:document.querySelector(n),context:N,flags:{}}),console.log(N)},P=function(){j.destroy(),Object(c.a)(),N.close()},L=t(162),$=t(161),S=t(160),W=t(159),T=t(158),F={components:{ContentHeading:L.a,ContentSubHeading:$.a,ContentSection:S.a,ContentCode:W.a,ContentNotice:T.a},mounted:function(){M("#flow")},beforeDestroy:function(){P()}},G=t(17),component=Object(G.a)(F,function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",[t("content-heading",[n._v("00-polysynth")]),n._v(" "),t("content-section",[t("content-sub-heading"),n._v("\n    Here's a simple polysynth. You can use the top row of your keyboard to\n    play some notes!\n\n    "),t("div",{staticClass:"border-4 my-4 p-2 border-gray-900",attrs:{id:"flow"}})],1),n._v(" "),t("content-section",[n._v("\n    The code for this is easy to break down. First we define a function to\n    create a note object to keep track of what key is pressed and what pitch\n    that key corresponds to:\n\n    "),t("content-code",[n._v("\n      const note = (key, name) => ({\n        key, \n        freq: Music.Note.ntof(name),\n        on: false\n      })\n    ")]),n._v("\n\n    We're using the ntof function in Flow's Music.Note package to convert note\n    names like 'A4' to frequencies like 440.\n  ")],1),n._v(" "),t("content-section",[n._v("\n    Then we can populate our model with an array of notes corresponding to the\n    top row of the keyboard. This inital model will get generated when we first\n    start the application:\n\n    "),t("content-code",[n._v("\n      function init (flags) {\n        return {\n          notes: [\n            note('q', 'A3'),\n            note('w', 'C4'),\n            note('e', 'D4'),\n            note('r', 'E4'),\n            note('t', 'G4'),\n            note('y', 'A4'),\n            note('u', 'C5'),\n            note('i', 'D5'),\n            note('o', 'E5'),\n            note('p', 'G5'),\n          ]\n        }\n      }\n    ")])],1),n._v(" "),t("content-section",[n._v("\n    We'll define two Actions for NoteOn and NoteOff events:\n\n    "),t("content-code",[n._v("\n      const NoteOn  = 'note-on'\n      const NoteOff = 'note-off'\n    ")]),n._v("\n\n    When dispatching those actions we'll also send along a payload with the\n    current key being pressed. Using that we can simply map over the notes\n    array and toggle a note on or off when appropriate:\n\n    "),t("content-code",[n._v("\n      function update ({ action, payload }, model) {\n        switch (action) {\n          case NoteOn: {\n            const { key } = payload\n            const notes = model.notes.map(note => note.key === key\n              ? { ...note, on: true }\n              : { ...note }\n            )\n\n            return [{ ...model, notes },\n              null\n            ]\n          }\n\n          case NoteOff: {\n            const { key } = payload\n            const notes = model.notes.map(note => note.key === key\n              ? { ...note, on: false }\n              : { ...note }\n            )\n\n            return [{ ...model, notes },\n              null\n            ]\n          }\n        }\n      }\n    ")]),n._v("\n\n    You'll note that we use the spread operatoe when returning the new model.\n    This isn't strictly necessary as we only have one field right now, but\n    doing this makes it easier to add new fields to the model at a later date.\n  ")],1),n._v(" "),t("content-section",[n._v("\n    We also return the model in an array with null as the second element. This\n    indicates to the runtime that there are no Effects to run. It's good\n    practice to do this for clarity.\n  ")]),n._v(" "),t("content-section",[n._v("\n    For each note we're going to create a separate voice of our synth, this\n    is how we'll get polyphony.\n\n    "),t("content-code",[n._v("\n      const voice = ({ on, freq }) => {\n        const vol   = Property.gain(on ? 0.1 : 0)\n        const pitch = Property.frequency(freq)\n\n        return (\n          Node.oscillator([ pitch ], [\n            Node.gain([ vol ], [\n              Node.dac()\n            ])\n          ])\n        )\n      }\n    ")]),n._v("\n\n    If Flow's declarative audio API is doing its job properly, the above\n    snippet should be fairly self explanatory. For each note we create an\n    oscillator, connect it to a gain node, and then connect that to the\n    output. \n  ")],1),n._v(" "),t("content-section",[n._v("\n    We the voice function defined, our audio function is a dead simple\n    one-liner:\n\n    "),t("content-code",[n._v("\n      function audio (model) {\n        return model.notes.map(voice)\n      }\n    ")]),n._v("\n\n    Easy!\n  ")],1),n._v(" "),t("content-section",[n._v("\n    We can do the same thing for our view. We'll have a function that takes\n    a note and uses the `on` property to toggle some different CSS classes:\n\n    "),t("content-code",[n._v("\n      const keyView = ({ key, freq, on }) => {\n        const classes = `\n          flex-1 text-center p-2 m-2 text-${on ? 'indigo-700' : 'gray-400'}\n          border border-${on ? 'indigo-700' : 'gray-400'}\n        `\n\n        return (\n          Element.div([ Attribute.className(classes) ], [\n            Element.p([], [ key ]),\n            Element.p([], [ Music.Note.fton(freq) ])\n          ])\n        )\n      }\n    ")]),n._v("\n\n    You can see we're using the fton function to convert the frequency back\n    to a note name. The Music.Note library comes with a bunch of functions to\n    convert to and from different types.\n  ")],1),n._v(" "),t("content-section",[n._v("\n    And just like the audio function, our view function is super simple. We\n    just need to wrap our rendered notes in a div and away we go:\n\n    "),t("content-code",[n._v("\n      function view (model) {\n        return (\n          Element.div([ Attribute.className('flex border-4 my-4 p-2 border-gray-900') ],\n            model.notes.map(keyView)\n          )\n        )\n      }\n    ")])],1),n._v(" "),t("div",{staticClass:"{-typography text-right text-indigo-500 hover:text-indigo-800 -} {-spacing my-4 -}"},[t("nuxt-link",{attrs:{to:"/examples/01-step-sequencer"}},[n._v("01-step-sequencer")]),n._v(" »\n  ")],1)],1)},[],!1,null,null,null);e.default=component.exports}}]);